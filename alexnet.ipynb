{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "##import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation,GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"C:\\\\Users\\\\farha\\\\Desktop\\\\Py\\\\train\"\n",
    "\n",
    "test_path = \"C:\\\\Users\\\\farha\\\\Desktop\\\\Py\\\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class             Count:\n",
      "------------------------\n",
      "Crayons_train        104\n",
      "Oil Pastle_train     1368\n",
      "Outline_train        369\n",
      "Painting_train       230\n",
      "Pencil shading_train 233\n",
      "------------------------\n",
      "Number of classes :  5\n"
     ]
    }
   ],
   "source": [
    "#Getting the count of images for each class\n",
    "image_count = []\n",
    "class_names = []\n",
    "print('{:18s}'.format('class'), end='')\n",
    "print('Count:')\n",
    "print('-' * 24)\n",
    "#Reading the image from each folder from training path\n",
    "for folder in os.listdir(train_path):\n",
    "    folder_num = len(os.listdir(os.path.join(train_path,folder)))\n",
    "    image_count.append(folder_num)\n",
    "    class_names.append(folder)\n",
    "    print('{:20s}'.format(folder), end=' ')\n",
    "    print(folder_num)\n",
    "print('-' * 24)    \n",
    "print(\"Number of classes : \",len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "className = glob(train_path + '/*')\n",
    "num_classes = len(className)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This model consist of 5 convolution layers followed by 3 fully connected layers. Ativation function used is relu. \n",
    "##The network consists of a kernel or filters with size 11 x 11, 5 x 5, 3 x 3, 3 x 3 and 3 x 3 for its five convolutional layers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "   try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "   except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    #1st Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #2nd Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #3rd Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #4th Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    #5th Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    #Passing it to a Fully Connected layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 1st Fully Connected Layer\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),# Add Dropout to prevent overfitting\n",
    "    # 2nd Fully Connected Layer\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # 3rd Fully Connected Layer\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    #Output Layer\n",
    "    tf.keras.layers.Dense(5, activation='softmax'),\n",
    "    #tf.keras.layers.BatchNormalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.00001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 384)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6, 6, 384)        1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4096)             16384     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4097000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 5005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,405,237\n",
      "Trainable params: 62,394,293\n",
      "Non-trainable params: 10,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Getting model's summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifing epochs & batch size\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "image_height = 227\n",
    "image_width = 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an object of ImageDataGenerator for augmenting training dataset\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255,\n",
    "rotation_range=10,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.1,\n",
    "zoom_range=0.1,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "\n",
    "#Creating an object of ImageDataGenerator for augmenting test dataset\n",
    "test_datagen = ImageDataGenerator(rescale= 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2304 images belonging to 5 classes.\n",
      "Found 758 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Generating batches of Augmented data of image size 227, 227 and batch size of 32\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "directory = train_path, \n",
    "target_size= (image_height, image_width), # resize to this size\n",
    "batch_size = batch_size,\n",
    "color_mode= \"rgb\",\n",
    "class_mode= \"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "directory = test_path,\n",
    "target_size=(image_height, image_width),\n",
    "batch_size = batch_size,\n",
    "color_mode= \"rgb\",\n",
    "class_mode= \"categorical\")\n",
    "\n",
    "nb_train_samples = train_generator.samples\n",
    "nb_test_samples = test_generator.samples\n",
    "classes = list(train_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#Callback to save the best model. Using checkpoint and earlystopping to monitor validation accuracy\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.1, patience=10, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='fruit_model.h5',\n",
    "        monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.3739 - accuracy: 0.4562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.48098, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 292s 1s/step - loss: 1.3739 - accuracy: 0.4562 - val_loss: 1.3598 - val_accuracy: 0.4810 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.2261 - accuracy: 0.5551\n",
      "Epoch 2: val_accuracy improved from 0.48098 to 0.62908, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 277s 963ms/step - loss: 1.2261 - accuracy: 0.5551 - val_loss: 1.0753 - val_accuracy: 0.6291 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.1695 - accuracy: 0.5911\n",
      "Epoch 3: val_accuracy improved from 0.62908 to 0.63451, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 298s 1s/step - loss: 1.1695 - accuracy: 0.5911 - val_loss: 1.0472 - val_accuracy: 0.6345 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.1409 - accuracy: 0.6085\n",
      "Epoch 4: val_accuracy improved from 0.63451 to 0.65353, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 276s 957ms/step - loss: 1.1409 - accuracy: 0.6085 - val_loss: 1.0159 - val_accuracy: 0.6535 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.6120\n",
      "Epoch 5: val_accuracy improved from 0.65353 to 0.66712, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 261s 905ms/step - loss: 1.0763 - accuracy: 0.6120 - val_loss: 0.9632 - val_accuracy: 0.6671 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.6228\n",
      "Epoch 6: val_accuracy did not improve from 0.66712\n",
      "288/288 [==============================] - 253s 877ms/step - loss: 1.0732 - accuracy: 0.6228 - val_loss: 0.9347 - val_accuracy: 0.6603 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.0258 - accuracy: 0.6385\n",
      "Epoch 7: val_accuracy improved from 0.66712 to 0.70652, saving model to fruit_model.h5\n",
      "288/288 [==============================] - 247s 857ms/step - loss: 1.0258 - accuracy: 0.6385 - val_loss: 0.9060 - val_accuracy: 0.7065 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.0444 - accuracy: 0.6319\n",
      "Epoch 8: val_accuracy did not improve from 0.70652\n",
      "288/288 [==============================] - 245s 852ms/step - loss: 1.0444 - accuracy: 0.6319 - val_loss: 0.8896 - val_accuracy: 0.6793 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 1.0082 - accuracy: 0.6484\n",
      "Epoch 9: val_accuracy did not improve from 0.70652\n",
      "288/288 [==============================] - 243s 844ms/step - loss: 1.0082 - accuracy: 0.6484 - val_loss: 0.8937 - val_accuracy: 0.6929 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.9888 - accuracy: 0.6584\n",
      "Epoch 10: val_accuracy did not improve from 0.70652\n",
      "288/288 [==============================] - 243s 845ms/step - loss: 0.9888 - accuracy: 0.6584 - val_loss: 0.8674 - val_accuracy: 0.6957 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model\n",
    "#Training\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data=test_generator,\n",
    "        verbose = 1,\n",
    "        validation_steps=nb_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"AlexNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a62bb5b025c64fbc38028481dc970427ccecd3428447d4198e332bbd02098a56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
