{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import Xception\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using image_dataset_from_directory()\n",
    "data_dir = \"C:\\\\Users\\\\farha\\\\Desktop\\\\Py\\\\Pencil_shading\"\n",
    "image_size = (224, 224)\n",
    "batch_size = 20\n",
    "class_names_list = np.array(tf.io.gfile.listdir(data_dir)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available class names:\n",
      "['Crayons', 'Oil Pastel', 'Outline', 'Painting', 'Pencil shading']\n"
     ]
    }
   ],
   "source": [
    "if class_names_list is not None and len(class_names) > 0:\n",
    "    print(\"Available class names:\")\n",
    "    print(class_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3061 files belonging to 5 classes.\n",
      "Using 2449 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    class_names=class_names_list,\n",
    "    validation_split=0.2,  # 80% training, 20% validation\n",
    "    subset='training',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3061 files belonging to 5 classes.\n",
      "Using 612 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    class_names=class_names_list,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the data pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator for data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Apply data augmentation to the training dataset\n",
    "train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using sklearn\n",
    "X = []\n",
    "y = []\n",
    "for images, labels in train_data:\n",
    "    X.append(images.numpy())\n",
    "    y.append(labels.numpy())\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Xception model\n",
    "base_model = Xception(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size[0], image_size[1], 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification head\n",
    "inputs = tf.keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "x = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('xception_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_names_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(img)\n\u001b[0;32m      7\u001b[0m class_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m predicted_labels \u001b[39m=\u001b[39m [class_names_list[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m class_indices]\n\u001b[0;32m      9\u001b[0m \u001b[39m#['Crayons', 'Oil Pastel', 'Outline', 'Painting', 'Pencil shading']\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(predicted_labels)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(img)\n\u001b[0;32m      7\u001b[0m class_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m predicted_labels \u001b[39m=\u001b[39m [class_names_list[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m class_indices]\n\u001b[0;32m      9\u001b[0m \u001b[39m#['Crayons', 'Oil Pastel', 'Outline', 'Painting', 'Pencil shading']\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(predicted_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_names_list' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = \"C:\\\\Users\\\\farha\\\\Desktop\\\\test images\\\\p1.jpg\"\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224,224))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "predictions = model.predict(img)\n",
    "class_indices = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [class_names_list[i] for i in class_indices]\n",
    "#['Crayons', 'Oil Pastel', 'Outline', 'Painting', 'Pencil shading']\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "123/123 [==============================] - ETA: 0s - loss: 1.1888 - accuracy: 0.5947"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nSameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;28a3d894238b4eb9;/job:localhost/replica:0/task:0/device:GPU:0;edge_242_IteratorGetNext;0:0\n\t [[{{node IteratorGetNext/_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_test_function_12137]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      5\u001b[0m     X_train, y_train,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      8\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test, y_test)\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\farha\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\farha\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nSameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;28a3d894238b4eb9;/job:localhost/replica:0/task:0/device:GPU:0;edge_242_IteratorGetNext;0:0\n\t [[{{node IteratorGetNext/_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_test_function_12137]"
     ]
    }
   ],
   "source": [
    "# # Train the model\n",
    "# epochs = 10\n",
    "# batch_size = 16\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('xception_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
